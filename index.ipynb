{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajoutez ceci avant les imports\n",
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import os  # Pour gérer les fichiers et dossiers\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader  # Pour charger les fichiers Markdown\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Pour découper les documents\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # Pour générer les embeddings\n",
    "from langchain_community.vectorstores import FAISS  # Pour stocker les embeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaEmbeddings  # Pour utiliser le modèle mistral:7b\n",
    "from langchain.chains import create_retrieval_chain  # Pour la chaîne de récupération\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # Pour combiner les documents\n",
    "from langchain_core.documents import Document  # Pour représenter les documents\n",
    "from langchain.prompts import PromptTemplate  # Pour personnaliser le prompt\n",
    "from sentence_transformers import SentenceTransformer, util  # Pour le re-ranking avec BAAI/bge-m3\n",
    "import numpy as np  # Pour calculer les similarités\n",
    "from langchain_core.retrievers import BaseRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 1 : Configuration\n",
    "# ---------------------------------------\n",
    "\n",
    "#paramètres importants pour le script\n",
    "CONFIG = {\n",
    "    \"markdown_dir\": \"./markdown_branchements\",  # Dossier des fichiers Markdown\n",
    "    \"faiss_dir\": \"./faiss_index\",  # Dossier pour la base FAISS\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Modèle pour l'indexation\n",
    "    \"reranker_model\": \"bge-m3:latest\",  # Modèle pour le re-ranking\n",
    "    \"ollama_model\": \"mistral:7b\",  # Modèle de langage\n",
    "    \"top_k_retrieve\": 10,  # Documents à récupérer\n",
    "    \"top_k_rerank\": 3,  # Documents après re-ranking\n",
    "    \"chunk_size\": 512,  # Taille des morceaux\n",
    "    \"chunk_overlap\": 50  # Chevauchement des morceaux\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 2 : Classe pour charger les fichiers\n",
    "# ---------------------------------------\n",
    "class RobustTextLoader(TextLoader):\n",
    "    \"\"\"Classe pour lire les fichiers Markdown avec gestion des encodages.\"\"\"\n",
    "    def __init__(self, file_path, encoding=\"utf-8\", fallback_encodings=[\"iso-8859-1\", \"cp1252\", \"utf-16\"]):\n",
    "        super().__init__(file_path, encoding=encoding)\n",
    "        self.fallback_encodings = fallback_encodings\n",
    "\n",
    "    def lazy_load(self):\n",
    "        for encoding in [self.encoding] + self.fallback_encodings:\n",
    "            try:\n",
    "                with open(self.file_path, encoding=encoding) as f:\n",
    "                    text = f.read()\n",
    "                yield Document(page_content=text, metadata={\"source\": self.file_path})\n",
    "                return\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        print(f\"Échec du chargement de {self.file_path} : impossible de décoder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template utilisé :\n",
      "\n",
      "Tu es un assistant expert du processus de branchement chez ENEO. Tes réponses doivent être :\n",
      "- Précises et basées exclusivement sur les documents fournis\n",
      "- En français courant et facile à comprendre\n",
      "- Structurées avec des listes à puces quand c'est pertinent\n",
      "- Précise le nom des documents d'où proviennent tes réponses\n",
      "\n",
      "Contexte :\n",
      "{context}\n",
      "\n",
      "Question :\n",
      "{input}\n",
      "\n",
      "Réponds en t'appuyant sur le contexte fourni. Si tu ne sais pas, dis que tu n'as pas l'information.\n",
      "\n",
      "Variables attendues : ['context', 'input']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 3 : Prompt personnalisé\n",
    "# ---------------------------------------\n",
    "# On définit un modèle de prompt pour que les réponses soient claires et structurées\n",
    "PROMPT = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Tu es un assistant expert du processus de branchement chez ENEO. Tes réponses doivent être :\n",
    "- Précises et basées exclusivement sur les documents fournis\n",
    "- En français courant et facile à comprendre\n",
    "- Structurées avec des listes à puces quand c'est pertinent\n",
    "- Précise le nom des documents d'où proviennent tes réponses\n",
    "\n",
    "Contexte :\n",
    "{context}\n",
    "\n",
    "Question :\n",
    "{input}\n",
    "\n",
    "Réponds en t'appuyant sur le contexte fourni. Si tu ne sais pas, dis que tu n'as pas l'information.\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"input\"]\n",
    ")\n",
    "\n",
    "# Vérification du prompt pour diagnostiquer les erreurs\n",
    "print(\"Prompt template utilisé :\")\n",
    "print(PROMPT.template)\n",
    "print(\"Variables attendues :\", PROMPT.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 4 :  l'indexation\n",
    "# ---------------------------------------\n",
    "def setup_search_embeddings():\n",
    "    \"\"\"Configure le modèle d’embeddings pour la recherche.\"\"\"\n",
    "    print(\"Configuration du modèle d'embeddings...\")\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=CONFIG[\"embedding_model\"],\n",
    "        model_kwargs={\"device\": \"cpu\"}\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 5 :  chargement des documents\n",
    "# ---------------------------------------\n",
    "\n",
    "def load_documents():\n",
    "    \"\"\"Charge les fichiers Markdown.\"\"\"\n",
    "    print(\"Chargement des fichiers Markdown...\")\n",
    "    loader = DirectoryLoader(\n",
    "        CONFIG[\"markdown_dir\"],\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=RobustTextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "        show_progress=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"{len(documents)} documents chargés :\")\n",
    "    for doc in documents:\n",
    "        excerpt = doc.page_content[:100].replace(\"\\n\", \" \")\n",
    "        print(f\" - {doc.metadata['source']} (extrait : {excerpt}...)\")\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------\n",
    "# SECTION 6 :  découpage des documents\n",
    "# ---------------------------------------\n",
    "def split_documents(documents):\n",
    "    \"\"\"Découpe les documents en morceaux.\"\"\"\n",
    "    print(\"Découpage des documents en morceaux...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CONFIG[\"chunk_size\"],\n",
    "        chunk_overlap=CONFIG[\"chunk_overlap\"]\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"{len(chunks)} morceaux créés.\")\n",
    "    return chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 7 :  création de la base vectorielle\n",
    "# ---------------------------------------\n",
    "def create_vectorstore(chunks, embedding_model):\n",
    "    \"\"\"Crée et sauvegarde la base vectorielle FAISS.\"\"\"\n",
    "    print(\"Création de la base vectorielle...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    vectorstore.save_local(CONFIG[\"faiss_dir\"])\n",
    "    print(f\"Base vectorielle sauvegardée dans {CONFIG['faiss_dir']}.\")\n",
    "    return vectorstore\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------\n",
    "# SECTION 8 :  chargement de la base vectorielle\n",
    "# ---------------------------------------\n",
    "def load_vectorstore(embedding_model):\n",
    "    \"\"\"Charge la base vectorielle FAISS existante.\"\"\"\n",
    "    print(f\"Chargement de la base vectorielle depuis {CONFIG['faiss_dir']}...\")\n",
    "    vectorstore = FAISS.load_local(CONFIG[\"faiss_dir\"], embedding_model, allow_dangerous_deserialization=True)\n",
    "    print(\"Base vectorielle chargée.\")\n",
    "    return vectorstore\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 9 :  configuration du re-ranking\n",
    "# ---------------------------------------\n",
    "\n",
    "def setup_reranker():\n",
    "    \"\"\"Configure le modèle de re-ranking avec Ollama.\"\"\"\n",
    "    print(\"Configuration du re-ranker...\")\n",
    "    return OllamaEmbeddings(model=CONFIG[\"reranker_model\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------\n",
    "# SECTION 10 :  création du retriever (récupère les doc de ma base et les reclasse avec reranker_model)\n",
    "# ---------------------------------------\n",
    "\n",
    "def create_custom_retriever(vectorstore, reranker_model):\n",
    "    \"\"\"Crée un retriever avec re-ranking compatible \"\"\"\n",
    "    class CustomRetriever(BaseRetriever):\n",
    "        vectorstore: FAISS\n",
    "        reranker_model: OllamaEmbeddings\n",
    "        top_k_retrieve: int\n",
    "        top_k_rerank: int\n",
    "\n",
    "        def _get_relevant_documents(self, query: str) -> list[Document]:\n",
    "            # Récupérer les documents initiaux\n",
    "            initial_docs = self.vectorstore.similarity_search(query, k=self.top_k_retrieve)\n",
    "            # Générer l’embedding de la question\n",
    "            query_embedding = np.array(self.reranker_model.embed_query(query))\n",
    "            # Générer les embeddings des documents\n",
    "            doc_embeddings = np.array([self.reranker_model.embed_query(doc.page_content) for doc in initial_docs])\n",
    "            # Calculer les similarités cosinus\n",
    "            similarities = np.dot(doc_embeddings, query_embedding) / (\n",
    "                np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "            )\n",
    "            # Trier par similarité\n",
    "            scored_docs = list(zip(initial_docs, similarities))\n",
    "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "            # Retourner les meilleurs documents\n",
    "            return [doc for doc, _ in scored_docs[:self.top_k_rerank]]\n",
    "\n",
    "    return CustomRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        reranker_model=reranker_model,\n",
    "        top_k_retrieve=CONFIG[\"top_k_retrieve\"],\n",
    "        top_k_rerank=CONFIG[\"top_k_rerank\"]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 11 :  configuration du LLM\n",
    "# ---------------------------------------\n",
    "def setup_llm():\n",
    "    \"\"\"Configure le modèle de langage.\"\"\"\n",
    "    print(f\"Configuration du LLM {CONFIG['ollama_model']}...\")\n",
    "    llm = Ollama(model=CONFIG[\"ollama_model\"])\n",
    "    print(\"LLM configuré.\")\n",
    "    return llm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------\n",
    "# SECTION 12 :  création de la chaîne QA\n",
    "# ---------------------------------------\n",
    "\n",
    "def create_chain(llm, retriever):\n",
    "    \"\"\"Crée la chaîne de question-réponse.\"\"\"\n",
    "    print(\"Création de la chaîne de question-réponse...\")\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm, PROMPT)\n",
    "    chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "    print(\"Chaîne créée avec succès.\")\n",
    "    return chain\n",
    "\n",
    "# test de la chaine\n",
    "\n",
    "def test_chain(chain, question):\n",
    "    \"\"\"Teste la chaîne avec une question.\"\"\"\n",
    "    print(f\"\\nQuestion : {question}\")\n",
    "    try:\n",
    "        response = chain.invoke({\"input\": question})\n",
    "        print(f\"Réponse : {response['answer']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'invocation de la chaîne : {str(e)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 13 :  interactoin avec le LLM\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "def interactive_mode(chain):\n",
    "    \"\"\"Lance le mode interactif pour poser des questions.\"\"\"\n",
    "    print(\"\\nMode interactif activé. Tapez 'quitter' pour arrêter.\")\n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\nPosez une question : \")\n",
    "            if question.lower() == \"quitter\":\n",
    "                break\n",
    "            print(f\"Question : {question}\")\n",
    "            response = chain.invoke({\"input\": question})\n",
    "            print(f\"Réponse : {response['answer']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur dans le mode interactif : {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 14 :  main\n",
    "# ---------------------------------------\n",
    "def main():\n",
    "    \"\"\"Orchestre le programme avec toutes les  fonctions précedemment définies.\"\"\"\n",
    "    try:\n",
    "        # Configurer les composants\n",
    "        embedding_model = setup_search_embeddings()\n",
    "        \n",
    "        # Charger ou créer la base vectorielle\n",
    "        vectorstore = (load_vectorstore(embedding_model) if os.path.exists(CONFIG[\"faiss_dir\"])\n",
    "                       else create_vectorstore(split_documents(load_documents()), embedding_model))\n",
    "        \n",
    "        # Configurer le re-ranker et le retriever\n",
    "        reranker_model = setup_reranker()\n",
    "        retriever = create_custom_retriever(vectorstore, reranker_model)\n",
    "        \n",
    "        # Configurer le LLM et la chaîne\n",
    "        llm = setup_llm()\n",
    "        chain = create_chain(llm, retriever)\n",
    "        \n",
    "        # Tester et passer en mode interactif\n",
    "        test_chain(chain, \"decris moi la marche à suivre pour effectuer un nouveau branchement ?\")\n",
    "        interactive_mode(chain)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue : {str(e)}\")\n",
    "    finally:\n",
    "        print(\"\\nProgramme terminé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration du modèle d'embeddings...\n",
      "Chargement de la base vectorielle depuis ./faiss_index...\n",
      "Base vectorielle chargée.\n",
      "Configuration du re-ranker...\n",
      "Configuration du LLM deepseek-r1:14b...\n",
      "LLM configuré.\n",
      "Création de la chaîne de question-réponse...\n",
      "Chaîne créée avec succès.\n",
      "\n",
      "Question : decris moi la marche à suivre pour effectuer un nouveau branchement ?\n",
      "Réponse : <think>\n",
      "Okay, so I'm trying to figure out the process for a new branch connection based on the information provided. Let me start by looking at the context given.\n",
      "\n",
      "The context mentions two main points:\n",
      "\n",
      "1. Every month, the company must send detailed statements of connections made and the amounts spent by them for these works no later than the 15th of the following month.\n",
      "2. They need to recover the amounts from households connected as per the revolving fund agreement signed by the beneficiary.\n",
      "\n",
      "Additionally, there's a commercial component related to new prepaid connections.\n",
      "\n",
      "So, based on this, I can outline some steps, but I'm not entirely sure if I'm missing anything because the information is quite limited.\n",
      "\n",
      "First, probably they need to assess the customer's eligibility. That makes sense because not everyone might qualify for the connection or the revolving fund.\n",
      "\n",
      "Next, after determining eligibility, they would likely install the necessary equipment and infrastructure required for the new connection. This includes any meters, cables, etc., depending on what ENEO uses.\n",
      "\n",
      "Then, there must be a process to connect the household to the grid. This could involve physical work like linking the wires and setting up the metering system.\n",
      "\n",
      "After connecting, they need to provide detailed statements each month. The customer should receive these by the 15th of the following month, which would allow them time to review and make payments if needed.\n",
      "\n",
      "Recovery is another key part. They have to ensure that customers pay back the amounts provided under the revolving fund. This probably involves regular follow-ups or some form of payment reminders.\n",
      "\n",
      "Lastly, there might be a process for disconnecting if payments aren't made on time, but this isn't explicitly mentioned in the context.\n",
      "\n",
      "I think I covered all the points from the context, but I'm not entirely sure if there are more steps involved beyond what's provided. Maybe things like initial application submission, site inspection, or contract signing could be part of it, but these aren't mentioned here.\n",
      "</think>\n",
      "\n",
      "The process for a new branch connection at ENEO, based on the provided context, can be outlined as follows:\n",
      "\n",
      "1. **Eligibility Check**: Assess the customer's eligibility for the new connection and participation in the revolving fund.\n",
      "\n",
      "2. **Installation of Equipment**: Install necessary infrastructure such as meters and cables required for the connection.\n",
      "\n",
      "3. **Physical Connection**: Connect the household to the grid, ensuring all equipment is properly set up.\n",
      "\n",
      "4. **Monthly Statements**: Provide detailed statements by the 15th of each following month, detailing connections made and amounts spent.\n",
      "\n",
      "5. **Recovery Process**: Recover amounts from households in accordance with the revolving fund agreement, including follow-ups for payment.\n",
      "\n",
      "6. **Payment Monitoring**: Ensure timely payments; though disconnection procedures aren't mentioned here, they may be part of standard operations if applicable.\n",
      "\n",
      "Note: Additional steps like application submission or site inspections might exist but aren't detailed in the given context.\n",
      "\n",
      "Mode interactif activé. Tapez 'quitter' pour arrêter.\n",
      "Question : quelles sont les differentes etapes à suivre pour modifier un branchement\n",
      "Réponse : <think>\n",
      "Bon, je dois répondre à la question de quelles sont les différentes étapes à suivre pour modifier un branchement chez ENEO. Le contexte fourni par l'utilisateur se compose de plusieurs mentions de \"facture reçue de paiement du branchement à modifier\", ce qui suggère qu'il est en train d'essayer de comprendre ou de mener une procédure liée au changement de ses informations de paiement pour le branchement.\n",
      "\n",
      "Comme assistant, je dois me baser uniquement sur les documents fournis. Malheureusement, dans ce cas, il semble que les seuls \"documents\" soient ces mentions de factures. Il n'y a pas de guide d'installation ou de procédure détaillée mentionnée explicitement.\n",
      "\n",
      "Je vais donc devoir inférer les étapes logiques qui pourraient être associées à une telle modification. Généralement, pour modifier un branchement, il est probable que le client doit contacter ENEO, fournir certaines informations, remplir des formalités administratives, et peut-être effectuer des paiements supplémentaires ou des modifications de contrat.\n",
      "\n",
      "Je vais structurer ces étapes en une liste à puces pour plus de clarté. Je mentionnerai chaque étape avec une branche expliquant ce qu'elle implique et le document d'où provient l'information, bien que dans ce cas, je doive avouer que les informations sont inférées而非 tirées directement des documents fournis.\n",
      "\n",
      "Je dois aussi m'assurer que la réponse est en français courant et facile à comprendre. En plus, je mentionnerai explicitement que je n'ai pas accès à des documents specifiques pour ce processus, donc l'information est basée sur des suppositions.\n",
      "\n",
      "En résumé, mon raisonnement consiste à identifier les étapes probables sans avoir de documentation spécifique, en utilisant le contexte limité fourni par l'utilisateur.\n",
      "</think>\n",
      "\n",
      "Il semble que les informations nécessaires pour décrire les étapes exactes à suivre pour modifier un branchement ne soient pas fournies dans les documents disponibles. Cependant, basée sur la pratique courante, voici une liste des étapes généralement impliquées :\n",
      "\n",
      "- **Contactez ENEO** : Pour informer de votre intention de modifier le branchement.\n",
      "- **Fournir les informations nécessaires** : Comme les coordonnées mises à jour ou d'autres détails.\n",
      "- **Paiement éventuel** : Si des frais s'appliquent pour le changement.\n",
      "- **Validation et mise en œuvre** : ENEO procède aux modifications.\n",
      "\n",
      "Il est important de noter que ces étapes sont générales et peuvent varier selon les conditions spécifiques. Pour une guidance précise, il est recommandé de se référer aux documents officiels fournis par ENEO.\n",
      "\n",
      "Programme terminé.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# SECTION 15 :  lancement du programme\n",
    "# ---------------------------------------\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
